{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch Data Processing - Batch processing of  Electronic Medical Reports (EMR) with Amazon Comprehend Medical\n",
    "\n",
    "In the previous module, [1.Data_Processing](./1.Data_processing.ipynb), we learnt how to extract medical information from a single PDF medical report using Textract and Comprehend Medical. In this step, we will be preparing our dataset for the classification machine learning model that we will be building. We will use the same pre-processing methodology from the previous step to process a batch of medical reports. But to minimise processing time and cost for this lab, we will skipping the Textract step and focusing on be processing a batch of medical reports in textual format using Comprehend Medical. We use this output from Comprehend Medical as our dataset to train our machine learning model.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background)\n",
    "1. [Setup Environment](#Setup-Environment)\n",
    "1. [Load and Explore Data](#Load-and-Explore-Data)\n",
    "   + Load MTSamples data\n",
    "   + Remove samples with missing transcriptions\n",
    "   + Explore data distribution\n",
    "   + Extract medical information from data using Comprehend Medical\n",
    "   + Organize medical conditions\n",
    "1. [Data Sampling](#Data-Sampling)\n",
    "   + Sampling for patients for Surgery\n",
    "   + Sampling for patients for consultation\n",
    "1. [Combine samples](#Combine-the-dataset)\n",
    "1. [Save the File](#Save-the-File)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective \n",
    "This notebook is the preprocessing step to prepare a batch of medical records for model training. Specifically, you will use Comprehend Medical to extract medical terms from doctors's transcripts and organize them into data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "**Dataset**: Medical transcription data scraped from mtsamples.com. The target in the preprocess is to extract the medical conditions from doctors' notes, and then organized into dataset for modeling. In the next step, we will use the processed dataset to correctly classify the medical specialties based on the transcription text. In real life, the model can be used for automatic reference to respective specialist.\n",
    "\n",
    "**Amazon Comprehend Medical**: Comprehend Medical detects useful information in unstructured clinical text. As much as 75% of all health record data is found in unstructured text such as physician's notes, discharge summaries, test results, and case notes. Amazon Comprehend Medical uses Natural Language Processing (NLP) models to sort through text for valuable information.\n",
    "\n",
    "**Supported Languages**: Amazon Comprehend Medical only detects medical entities in English language texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Environment\n",
    "\n",
    "Before we begin, let us setup our environment, will be doing the following:\n",
    "\n",
    "- **import** some useful libraries (as in any Python notebook)\n",
    "- **configure** the S3 bucket and folder where data should be stored (to keep our environment tidy)\n",
    "- **connect** to Amazon Comprehend(with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)) and SageMaker in particular (with the [sagemaker SDK](https://sagemaker.readthedocs.io/en/stable/)), to use the cloud services\n",
    "\n",
    "Note: While `boto3` is the general AWS SDK for Python, `sagemaker` provides some powerful, higher-level interfaces designed specifically for ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For matrix operations and numerical processing\n",
    "import pandas as pd  # For munging tabular data\n",
    "import time\n",
    "import os\n",
    "from util.preprocess import *  # helper function for classification reports\n",
    "\n",
    "# setting up SageMaker parameters\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"emr-mtSample\"  # Location in the bucket to store our files\n",
    "sgmk_session = sagemaker.Session()\n",
    "\n",
    "sgmk_client = boto_session.client(\"sagemaker\")  ## API for sagemaker\n",
    "cm_client = boto3.client(service_name='comprehendmedical', use_ssl=True, region_name = 'us-east-1') ## API for comprehend medical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Explore Data\n",
    "\n",
    "For this workshop, we have already included the MTSamples dataset in your local folder (`./data/mtsample.csv`). You can find the raw dataset is available at [kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions). \n",
    "\n",
    "**Columns in the dataset**:\n",
    "\n",
    "* `description`: Short description of transcription (string)\n",
    "* `medical_specialty`: Medical specialty classification of transcription (string)\n",
    "* `sample_name`: Transcription title\n",
    "* `transcription`: Transcribed doctors' notes\n",
    "* `keywords`: Relevant keywords from transcription\n",
    "\n",
    "In the next section of our workshop, [3.Model building, training and deployment](./3.Model_building_training_and_development.ipynb), we will be building a machine learning model to classify medical reports based on the transcription. But before we can do that, we need to extract data from the MTSamples dataset. To train our model, we will be extracting the medical information from  the `transcription` column using Amazon Comprehend Medical. We will also be using the `medical_specialty`column as our labels for our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/mtsamples.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up dataset\n",
    "\n",
    "Before we process our dataset, let us check for empty columns and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0) ## check for missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore dataset by medical speciality\n",
    "\n",
    "Let us do some data exploration and observe the distribution of medical reports by medical speciality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add patient_id for reference\n",
    "df=df[df.transcription.isnull()==False].reset_index()\n",
    "df['id']=df.index\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.countplot(y='medical_specialty',order=df['medical_specialty'].value_counts().index, data=df)  #df.medical_specialty.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data-Sampling\n",
    "\n",
    "#### Data Sampling 1 - Surgery\n",
    "As we can observe in the chart above, we have 1000+ medical reports that have been classified under the **Surgery** medical speciality. Let us dive deeper and randomly select 200 patients from the **Surgery** category and extract the medical conditions using Amazon Comprehend Medical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nSample=20 ##<specify a number to process the medical terms in a batch >\n",
    "medical_specialty=' Surgery'\n",
    "df_phyList1,patient_ids=subpopulation_comprehend(df, medical_specialty,sampleSize=nSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions of medical conditions\n",
    "\n",
    "Now that we have extracted the medical conditions using Amazon Comprehend Medical, let us take a look at the most common medical conditions observed in the medical reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: do a batch processing using Amazon Comprehend Medical \n",
    "\n",
    "In this challenge, you are required to write your code within for-loop. You are expected to extract all the medical_conditions for each patient, together with the confidence score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to process a multiple records\n",
    "def extractMCbatch(transcriptionList,patientIDList):\n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    if(len(transcriptionList)!=len(patientIDList)):\n",
    "        return(\"Error! different length!\")\n",
    "    \n",
    "    ## In this for loop, gererate a wide dataframe with extracted medical condition from each item, together with the corresponding ID \n",
    "    for item,patient_id in zip(transcriptionList,patientIDList):\n",
    "        print(item)\n",
    "        print(patient_id)\n",
    "        \n",
    "        \n",
    "        ########################################################################################\n",
    "        #########################  wrtie your code here ########################################\n",
    "        ###################### hint1: check item['Entities'] ###################################\n",
    "        ############### remember: add patient_id together with the extracted medical condition##    \n",
    "        ########################################################################################\n",
    "    \n",
    "    # remove the duplicated entries \n",
    "    df_final=df_final.sort_values(by=['ID','MEDICAL_CONDITION']).drop_duplicates(['ID','MEDICAL_CONDITION'],keep='last')\n",
    "\n",
    "    #print(df_final.shape)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stucked? Check the recordings on the website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## organize into a dataframe\n",
    "df_extracted_surg=extractMCbatch(df_phyList1,patient_ids)\n",
    "## plot the results\n",
    "topN=20 ## the number for top conditions\n",
    "threshold_score=0.9\n",
    "mc_barplot(df_extracted_surg, threshold_score,topN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sampling 2 - *Consult - History and Phy*\n",
    "\n",
    "Let us pick another medical speciality we want to try and classify. Let us pick 200 medical reports from the next most popular medical speciality - *Consult - History and Phy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nSample=20 ##<specify a number to process the medical terms in a batch >\n",
    "medical_specialty=' Consult - History and Phy.'\n",
    "df_phyList,patient_ids=subpopulation_comprehend(df, medical_specialty,sampleSize=nSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## organize into a dataframe\n",
    "df_extracted_consult=extractMCbatch(df_phyList,patient_ids)\n",
    "## plot the results\n",
    "topN=20 ## the number for top conditions\n",
    "threshold_score=0.9\n",
    "mc_barplot(df_extracted_consult, threshold_score,topN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine-the-dataset\n",
    "\n",
    "Now that we've picked two medical speciality we want to classify, let us consolidate this data and save it for the next part of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "from  util import preprocess\n",
    "reload(preprocess)\n",
    "#from util.preprocess import subpopulation_comprehend,retrieve_mcList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcList1, df_grp1=preprocess.retrieve_mcList(df_extracted_surg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcList2, df_grp2=preprocess.retrieve_mcList(df_extracted_consult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp1['Label']=True # group one is Labeled as True \n",
    "df_grp2['Label']=False  # group two is Labeled as True \n",
    "\n",
    "\n",
    "df_combined=df_grp1.append(df_grp2) ## append two data frames \n",
    "mcLists=list(set(mcList1+mcList2))\n",
    "\n",
    "df_combined2=preprocess.df_mc_generator(df_combined,mcLists ,colname_other=['ID',\"Label\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined2.to_csv(\"./data/processed_combined_wide_v1.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
